# 逻辑回归需要掌握的知识点
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227104544726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
- 知道逻辑回归的损失函数
- 知道逻辑回归的优化方法
- 知道sigmoid函数
- 知道逻辑回归的应用场景
- 应用LogisticRegression实现逻辑回归预测
- 知道精确率、召回率指标的区别
- 知道如何解决样本不均衡情况下的评估
- 了解ROC曲线的意义说明AUC指标大小
- 应用classification_report实现精确率、召回率计算
- 应用roc_auc_score实现指标计算

@[toc]

# 逻辑回归介绍

逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归，但是它与回归之间有一定的联系。由于算法的简单和高效，在实际中应用非常广泛。

目标值是离散的，作用：二分类，A B，输出两个值。

## 1.逻辑回归思路
线性回归：h(w)=w1x1+w2x2+...+wnxn+b

改为二分类（让输出值变为二分类）：

**<font color=DarkTurquoise face="楷体">塑造一个新的函数，比如，x的定义域是负无穷到正无穷，但让值域是0-1的，这个是属于某个类别的概率，比如让他属于A。如果属于A的概率小于0.5，就判断他是B类。</font>**



## 2.逻辑回归的应用场景

- 广告点击率 **<font color=DarkTurquoise face="楷体">（用户是否点击广告）</font>**
- 是否为垃圾邮件
- 是否患病
- 金融诈骗
- 虚假账号 **<font color=DarkTurquoise face="楷体">（用户是否是正常用户）</font>**

看到上面的例子，我们可以发现其中的特点，那就是都属于两个类别之间的判断。逻辑回归就是解决二分类问题的利器

二分类算法：如预测是否为好人
## 3.逻辑回归的原理
**<font color=DarkTurquoise face="楷体">数学模型：线性回归+激活函数</font>**

要想掌握逻辑回归，必须掌握两点：

 逻辑回归中，其输入值是什么

 如何判断逻辑回归的输出

### 3.1输入
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227094339314.png)

逻辑回归的输入就是一个线性回归的结果。

### 3.2激活函数（增加非线性模型的拟合能力）
**<font color=DarkTurquoise face="楷体">本来的激活函数：</font>**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227102231744.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227102342453.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
- 线性回归的sigmoid函数
**<font color=DarkTurquoise face="楷体">把线性回归放进去了</font>**
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227094356645.png)

**<font color=DarkTurquoise face="楷体">定义域：负无穷到正无穷</font>**

**<font color=DarkTurquoise face="楷体">值域：（0，1）</font>**


- 判断标准

  - 回归的结果输入到sigmoid函数当中
  - 输出结果：[0, 1]区间中的一个概率值，默认为0.5为阈值
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227094413458.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
> 逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1(正例),另外的一个类别会标记为0(反例)。（方便损失计算）

输出结果解释(重要)：假设有两个类别A，B，并且假设我们的概率值为属于A(1)这个类别的概率值。现在有一个样本的输入到逻辑回归输出结果0.6，那么这个概率值超过0.5，意味着我们训练或者预测的结果就是A(1)类别。那么反之，如果得出结果为0.3那么，训练或者预测结果就为B(0)类别。

所以接下来我们回忆之前的线性回归预测结果我们用均方误差衡量，那如果对于逻辑回归，我们预测的结果不对该怎么去衡量这个损失呢？我们来看这样一张图

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227094424875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
**<font color=DarkTurquoise face="楷体">【逻辑回归的数学模型：线性回归+激活函数】</font>**
**<font color=DarkTurquoise face="楷体">第1个是一个特征矩阵</font>**
**<font color=DarkTurquoise face="楷体">第2个是一个权重矩阵</font>**
**<font color=DarkTurquoise face="楷体">回归那里是线性回归</font>**
**<font color=DarkTurquoise face="楷体">加入激活函数之后</font>**
**<font color=DarkTurquoise face="楷体">得到逻辑回归的结果：属于A的概率</font>**
**<font color=DarkTurquoise face="楷体">设定一个阈值，比如0.5，因为我们要进行二分类</font>**
**<font color=DarkTurquoise face="楷体">得到一个二分类预测结果</font>**
**<font color=DarkTurquoise face="楷体">与真实结果进行比较</font>**
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227104229189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)

那么如何去衡量逻辑回归的预测结果与真实结果的差异呢？
**<font color=DarkTurquoise size = 5 face="楷体">人工神经元：</font>**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227102919943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)

**<font color=DarkTurquoise face="楷体">第一步是线性回归的计算，f就是激活函数Sigmoid，还有一些别人激活函数（ReLu），从输入到输出，就是一个非线性的计算，每个神经元都是这样计算的</font>**
至此我们学习了逻辑回归的以下部分
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022710451426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
## 4.损失以及优化
**<font color=DarkTurquoise face="楷体">可训练参数还是线性回归部分的w和b</font>**
**<font color=DarkTurquoise face="楷体">激活函数没有需要训练的参数</font>**
**<font color=DarkTurquoise face="楷体">既然需要训练，那就要有损失函数</font>**
### 4.1损失

逻辑回归的损失，称之为**对数似然损失****<font color=DarkTurquoise face="楷体">（对数似然损失，这个名字要记住）</font>**，公式如下：

- 分开类别：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227094440282.png)

怎么理解单个的式子呢？这个要根据log的函数图像来理解

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227094458967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
**<font color=DarkTurquoise face="楷体">y=1的时候，损失越小，h（x）越接近1</font>**
**<font color=DarkTurquoise face="楷体">y=1的时候，损失越大，h（x）越远离1</font>**
- 综合完整损失函数

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227094507551.png)
**<font color=DarkTurquoise face="楷体">上面两个y的情况的合并就得到这个公式</font>**
> 看到这个式子，其实跟我们讲的信息熵类似。

接下来我们呢就带入上面那个例子来计算一遍，就能理解意义了。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022709451469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
**<font color=DarkTurquoise face="楷体">根据特征矩阵知道有五个样本</font>**
**<font color=DarkTurquoise face="楷体">通过线性回归得到五个结果</font>**
**<font color=DarkTurquoise face="楷体">增加激活函数，让他变成了逻辑回归，想要去得到二分类，输出了五个【概率】</font>**
**<font color=DarkTurquoise face="楷体">要将五个结果变成二分类，就需要设置一个阈值，才能得到二分类的逻辑结果</font>**
**<font color=DarkTurquoise face="楷体">但是我们现在不去设置阈值，因为我们要看损失，和真实的结果进行对比</font>**
**<font color=DarkTurquoise face="楷体">根据之前得到的损失函数</font>**
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227094507551.png)
**<font color=DarkTurquoise face="楷体">计算损失函数如下</font>**
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020022709451469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
yi是真实结果，h（x）是逻辑回归预测的结果
**<font color=DarkTurquoise face="楷体">详细手写过程如下：</font>**
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227111610442.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200227111622665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDU2MDQ1,size_16,color_FFFFFF,t_70)
> 我们已经知道，log(P), P值越大，结果越小，所以我们可以对着这个损失的式子去分析

### 4.2优化

同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，**提升原本属于1类别的概率，降低原本是0类别的概率。**
